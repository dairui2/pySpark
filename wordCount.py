from pyspark import SparkConf,SparkContextif __name__ == '__main__':    conf = SparkConf().setAppName("PySparkWordCount").setMaster("yarn").set("spark.executor.memory", "2g").set("spark.driver.memory", "1g")    sc = SparkContext(conf=conf)    fileRdd = sc.textFile("hdfs://hdp2:8020/spark3-history/students*")    wordsRdd = fileRdd.flatMap(lambda line: line.split(" "))    wordRdd = wordsRdd.map(lambda x: (x, 1))    resultRdd = wordRdd.reduceByKey(lambda a, b: a + b)    print(resultRdd.collect())